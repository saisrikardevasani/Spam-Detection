{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvuXmocVmw9V"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import sklearn\n",
        "import pickle\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YoAWWXhTm8BS",
        "outputId": "3b8ba889-db54-4ae0-a961-d9ce385d71a8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset if not already present\n",
        "dataset_url = 'https://raw.githubusercontent.com/kristinlussi/DATA_607/main/Project4/spam_ham_dataset.csv'\n",
        "dataset_file = 'spam_ham_dataset.csv'\n",
        "\n",
        "if not os.path.exists(dataset_file):\n",
        "    print('Downloading dataset...')\n",
        "    urllib.request.urlretrieve(dataset_url, dataset_file)\n",
        "    print('Download complete.')\n",
        "\n",
        "data = pd.read_csv(dataset_file, encoding='latin-1')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR2sz1RYniCX",
        "outputId": "18afb13a-076d-476d-88b8-2c572569995e"
      },
      "outputs": [],
      "source": [
        "data[1990:2000]\ndata['label'].value_counts()\nimport nltk\nnltk.download(\"punkt\")\nimport warnings\nwarnings.filterwarnings('ignore')\nham_words = ''\nspam_words = ''\n# Creating a corpus of spam messages\nfor val in data[data['label'] == 'spam'].text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    for words in tokens:\n        spam_words = spam_words + words + ' '\n\n# Creating a corpus of ham messages\nfor val in data[data['label'] == 'ham'].text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    for words in tokens:\n        ham_words = ham_words + words + ' '\nspam_wordcloud = WordCloud(width=500, height=300).generate(spam_words)\nham_wordcloud = WordCloud(width=500, height=300).generate(ham_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwaY7ioRnOTx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "fb53b5a6-b0b1-40ae-a5b7-2ca321d48b94"
      },
      "outputs": [],
      "source": [
        "#Spam Word cloud\n",
        "plt.figure( figsize=(10,8), facecolor='w')\n",
        "plt.imshow(spam_wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mZkVvz2nqsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "76f3acc0-5e2b-4833-93f7-e1eb440acb22"
      },
      "outputs": [],
      "source": [
        "#Creating Ham wordcloud\n",
        "plt.figure( figsize=(10,8), facecolor='g')\n",
        "plt.imshow(ham_wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n",
        "print(ham_wordcloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYJJ_oS4n18p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8d5407fe-6b38-47cb-a22b-c82a32e872f5"
      },
      "outputs": [],
      "source": [
        "data = data.replace(['ham','spam'],[0, 1])\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZZ0B0kmVq4v",
        "outputId": "f0301caf-ec24-4b3b-c745-70eaaba87e3a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#remove the punctuations and stopwords\n",
        "import string\n",
        "def text_process(text):\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    return \" \".join(text)\n",
        "\n",
        "data['text'] = data['text'].apply(text_process)\n",
        "data.head()\n",
        "text = pd.DataFrame(data['text'])\n",
        "label = pd.DataFrame(data['label'])\n",
        "## Counting how many times a word appears in the dataset\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "total_counts = Counter()\n",
        "for i in range(len(text)):\n",
        "    for word in text.values[i][0].split(\" \"):\n",
        "        total_counts[word] += 1\n",
        "\n",
        "print(\"Total words in data set: \", len(total_counts))\n",
        "# Sorting in decreasing order (Word with highest frequency appears first)\n",
        "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
        "print(vocab[:60])\n",
        "# Mapping from words to index\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "word2idx = {}\n",
        "\n",
        "#print vocab_size\n",
        "for i, word in enumerate(vocab):\n",
        "    word2idx[word]=i\n",
        "# Text to Vector\n",
        "def text_to_vector(text):\n",
        "    word_vector = np.zeros(vocab_size)\n",
        "    for word in text.split(\" \"):\n",
        "        if word2idx.get(word) is None:\n",
        "            continue\n",
        "        else:\n",
        "            word_vector[word2idx.get(word)] += 1\n",
        "    return np.array(word_vector)\n",
        "# Convert all titles to vectors\n",
        "word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n",
        "for i, (_, text_) in enumerate(text.iterrows()):\n",
        "    word_vectors[i] = text_to_vector(text_[0])\n",
        "\n",
        "word_vectors.shape\n",
        "#convert the text data into vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(data['text'])\n",
        "vectors.shape\n",
        "features = vectors\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, random_state=111)\n",
        "#import sklearn packages for building classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "#initialize multiple classification models\n",
        "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
        "knc = KNeighborsClassifier(n_neighbors=49)\n",
        "mnb = MultinomialNB(alpha=0.2)\n",
        "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
        "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "rfc = RandomForestClassifier(n_estimators=31, random_state=111)\n",
        "#create a dictionary of variables and models\n",
        "clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}\n",
        "#fit the data onto the models\n",
        "def train(clf, features, targets):\n",
        "    clf.fit(features, targets)\n",
        "\n",
        "def predict(clf, features):\n",
        "    return (clf.predict(features))\n",
        "pred_scores_word_vectors = []\n",
        "for k,v in clfs.items():\n",
        "    train(v, X_train, y_train)\n",
        "    pred = predict(v, X_test)\n",
        "    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))\n",
        "print(pred_scores_word_vectors)\n",
        "def find(x):\n",
        "    if x == 1:\n",
        "        print (\"Message is SPAM\")\n",
        "    else:\n",
        "        print (\"Message is NOT Spam\")\n",
        "newtext = [\" no 1 in offer \"]\n",
        "integers = vectorizer.transform(newtext)\n",
        "x = mnb.predict(integers)\n",
        "find(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}